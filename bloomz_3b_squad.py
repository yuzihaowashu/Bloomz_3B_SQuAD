# -*- coding: utf-8 -*-
"""Bloomz_3B_SQuAD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12frgL7CmVgBIXO4Fs2AGJefUqfzubkO8
"""

from datasets import load_dataset

squad = load_dataset("squad", split="train")
squad = squad.train_test_split(test_size=0.2)


from transformers import AutoTokenizer, AutoModelForQuestionAnswering

model_ckpt = "bigscience/bloomz-3b"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
base_model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)


def preprocess_function(examples):
    questions = [q.strip() for q in examples["question"]]
    inputs = tokenizer(
        questions,
        examples["context"],
        max_length=384,
        truncation="only_second",
        return_offsets_mapping=True,
        padding="max_length",
    )

    offset_mapping = inputs.pop("offset_mapping")
    answers = examples["answers"]
    start_positions = []
    end_positions = []

    for i, offset in enumerate(offset_mapping):
        answer = answers[i]
        start_char = answer["answer_start"][0]
        end_char = answer["answer_start"][0] + len(answer["text"][0])
        sequence_ids = inputs.sequence_ids(i)

        # Find the start and end of the context
        idx = 0
        while idx < len(sequence_ids) and sequence_ids[idx] != 1:
            idx += 1
        context_start = idx
        while idx < len(sequence_ids) and sequence_ids[idx] == 1:
            idx += 1
        context_end = idx - 1

        # If the answer is not fully inside the context, label it (0, 0)
        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:
            start_positions.append(0)
            end_positions.append(0)
        else:
            # Otherwise it's the start and end token positions
            idx = context_start
            while idx <= context_end and offset[idx][0] <= start_char:
                idx += 1
            start_positions.append(idx - 1)

            idx = context_end
            while idx >= context_start and offset[idx][1] >= end_char:
                idx -= 1
            end_positions.append(idx + 1)

    inputs["start_positions"] = start_positions
    inputs["end_positions"] = end_positions
    return inputs

tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad["train"].column_names)


from transformers import DefaultDataCollator

data_collator = DefaultDataCollator()


# Fine-tune base model
import copy
from transformers import TrainingArguments, Trainer

batch_size = 8
training_args = TrainingArguments(
    output_dir="fine-tuned-model",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=5,
    weight_decay=0.01,
)

finetuned_trainer = Trainer(
    model=copy.deepcopy(base_model),
    args=training_args,
    train_dataset=tokenized_squad["train"],
    eval_dataset=tokenized_squad["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

finetuned_trainer.train()


# Fine tune LoRA Model
from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model
lora_config = LoraConfig(
        lora_alpha=256,
        lora_dropout=0.05,
        r=512,
        bias="none",
        target_modules= ["query_key_value"],
)
lora_model = get_peft_model(copy.deepcopy(base_model), lora_config)

lora_training_args = TrainingArguments(
    output_dir="lora-model",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=5,
    weight_decay=0.01,
)

lora_trainer = Trainer(
    model=lora_model,
    args=lora_training_args,
    train_dataset=tokenized_squad["train"],
    eval_dataset=tokenized_squad["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

lora_trainer.train()

# Fine tune QLoRA model

from transformers import BitsAndBytesConfig
import torch

compute_dtype = getattr(torch, "float16")
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=compute_dtype
)

qlora_model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt, quantization_config=quantization_config, trust_remote_code=True)
qlora_model.gradient_checkpointing_enable()
qlora_model = prepare_model_for_kbit_training(qlora_model)
qlora_model = get_peft_model(qlora_model, lora_config)

qlora_training_args = TrainingArguments(
    output_dir="qlora-model",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=5,
    weight_decay=0.01,
)

qlora_trainer = Trainer(
    model=qlora_model,
    args=qlora_training_args,
    train_dataset=tokenized_squad["train"],
    eval_dataset=tokenized_squad["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

qlora_trainer.train()

# Evaluate
from transformers import pipeline
import pandas as pd
from bert_score import score


def evaluate(model, tokenizer, dataset, name):
    question_answerer = pipeline("question-answering", model=model, tokenizer=tokenizer)
    questions = dataset["question"]
    answers = dataset["answers"]
    context = dataset["context"]
    predictions = []
    for i in range(len(questions)):
        answer = question_answerer(question=questions[i], context=context[i])
        predictions.append(answer)

    answers = [answer["text"][0] for answer in answers]
    predictions = [prediction["answer"] for prediction in predictions]
    R, P, F1 = score(answers, predictions, lang="en")

    results = {
        "Recall": R.mean().item(),
        "Precision": P.mean().item(),
        "F1 Score": F1.mean().item(),
    }

    results_df = pd.DataFrame(results, index=[name])


    return results_df

base_model_df = evaluate(base_model, tokenizer, squad["test"], "Base")
fine_tuned_df = evaluate(finetuned_trainer.model, tokenizer, squad["test"], "Full Model")
lora_df = evaluate(lora_trainer.model, tokenizer, squad["test"], "LoRA")
qlora_df = evaluate(qlora_trainer.model, tokenizer, squad["test"], "QLoRA")
results_df = pd.concat([base_model_df,fine_tuned_df, lora_df, qlora_df], axis=0)

def calculate_model_size(model, debug=False):
    total_bytes = 0
    for name, param in model.named_parameters():
        param_size = param.nelement() * param.element_size()
        if debug:
          print(f"Name: {name}, Size: {param_size} bytes")
        total_bytes += param_size

    # Converting bytes to megabytes and gigabytes
    total_megabytes = total_bytes / (1024 ** 2)
    total_gigabytes = total_bytes / (1024 ** 3)
    return total_bytes, total_megabytes, total_gigabytes


_, _, base_size_gigabytes = calculate_model_size(base_model)
_, _, fine_tuned_size_gigabytes = calculate_model_size(finetuned_trainer.model)
_, _, lora_size_gigabytes = calculate_model_size(lora_trainer.model)
_, _, qlora_size_gigabytes = calculate_model_size(qlora_trainer.model)



results_df['Size (GB)'] = [base_size_gigabytes, fine_tuned_size_gigabytes, lora_size_gigabytes, qlora_size_gigabytes]

# Show the dataframe
results_df